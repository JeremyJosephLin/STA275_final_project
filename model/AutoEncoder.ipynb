{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9108310d",
   "metadata": {},
   "source": [
    "# AutoEncoder Fusion\n",
    "\n",
    "This notebook uses AutoEncoder to impute the missing modalities. Take patients MRI data source as an example. We first model MRI->MRI AE to find the latent space for MRI features for those who have MRI records. However, for those who don't, we utilize the UDS->MRI AE to project UDS onto the shared latent space between UDS and MRI. As a result, we can have the complete dataset for MRI. But following issues also need to be considered:\n",
    "- The AutoEncoder needs a lot hyperparameter tuning. Even though, the latent representation might not be accurate.\n",
    "- The decoder dimension is high-dimensional (UDS-89, MRI-155). AE might be unstable.\n",
    "- The distribution of MRI might be different depending on whether a patient has a MRI record or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936e7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.utils.data as Data\n",
    "from torch.nn import functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def seed_torch(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)#as reproducibility docs\n",
    "    torch.manual_seed(seed)# as reproducibility docs\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False# as reproducibility docs\n",
    "    torch.backends.cudnn.deterministic = True# as reproducibility docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb903b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44740, 89) (2873, 161) (2180, 7)\n"
     ]
    }
   ],
   "source": [
    "def load_data(impute_method = 'Mean-Mode'):\n",
    "    uds = pd.read_csv(\"../data/data_imputed/{}/uds.csv\".format(impute_method))\n",
    "    uds['datetime'] = pd.to_datetime(uds['datetime'])\n",
    "    uds = uds.dropna(subset=['EDUC'])\n",
    "    \n",
    "    mri = pd.read_csv(\"../data/data_imputed/{}/mri.csv\".format(impute_method))\n",
    "    mri['datetime'] = pd.to_datetime(mri['datetime'])\n",
    "    \n",
    "    csf = pd.read_csv(\"../data/data_imputed/{}/csf.csv\".format(impute_method))\n",
    "    return uds, mri, csf\n",
    "\n",
    "uds_dict = pd.read_csv(\"../data/data_dictionary/uds_feature_dictionary_cleaned.csv\")\n",
    "mri_dict = pd.read_csv(\"../data/data_dictionary/mri_feature_dictionary_cleaned.csv\") \n",
    "\n",
    "uds_drop_columns = ['NACCID', 'NACCADC', 'NACCVNUM', 'datetime', 'NACCUDSD', 'NACCALZP', 'NACCAD3', 'NACCAD5']\n",
    "mri_drop_columns = ['NACCID', 'NACCVNUM', 'datetime', 'datetime_UDS', 'timediff', 'within-a-year']\n",
    "csf_drop_columns = ['NACCID', 'CSFABMD', 'CSFTTMD', 'CSFPTMD']\n",
    "\n",
    "uds, mri, csf = load_data()\n",
    "print(uds.shape, mri.shape, csf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5290d75",
   "metadata": {},
   "source": [
    "## AutoEncoder for Dimension Reduction - UDS and MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5fa18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, e_dim, d_dim, seed=48):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        seed_torch(seed)\n",
    "        e_list = [nn.Linear(e_dim[0], e_dim[1])]\n",
    "        for i in range(2, len(e_dim)):\n",
    "            e_list.append(nn.ReLU())\n",
    "#             e_list.append(nn.Dropout(0.1))\n",
    "            e_list.append(nn.Linear(e_dim[i-1], e_dim[i]))\n",
    "        d_list = [nn.Linear(d_dim[0], d_dim[1])]\n",
    "        for i in range(2, len(d_dim)):\n",
    "            d_list.append(nn.ReLU())\n",
    "#             d_list.append(nn.Dropout(0.1))\n",
    "            d_list.append(nn.Linear(d_dim[i-1], d_dim[i]))\n",
    "        self.encoder_layers = nn.Sequential(*e_list)\n",
    "        self.decoder_layers = nn.Sequential(*d_list)\n",
    "    def encode(self, X):\n",
    "        return self.encoder_layers(X)\n",
    "    def decode(self, Z):\n",
    "        return self.decoder_layers(Z)\n",
    "    def loss(self, X1, X2):\n",
    "        Z = self.encoder_layers(X1)\n",
    "        X_hat = self.decoder_layers(Z)\n",
    "        return torch.mean((X2-X_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20f9e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_data(X, test_size = 0.2, seed=48):\n",
    "    X_train, X_test = train_test_split(X, test_size=test_size, random_state=seed)\n",
    "    X_val, X_test = train_test_split(X_test, test_size=test_size, random_state=seed)\n",
    "    X_train = torch.tensor(X_train).float().to(device)\n",
    "    X_val = torch.tensor(X_val).float().to(device)\n",
    "    X_test = torch.tensor(X_test).float().to(device)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "model = None\n",
    "\n",
    "def trainAE(X1, e_dim, d_dim, X2=None, test_size = 0.2, \n",
    "            lr=1e-2, epochs=200, weight_decay=1e-5, T_max=200, small=1e-7, \n",
    "            display_intvl=20, seed=48):\n",
    "    global model\n",
    "\n",
    "    model = AutoEncoder(e_dim, d_dim, seed=seed).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,  T_max=T_max)\n",
    "    if X2 is None:\n",
    "        X2 = X1\n",
    "    X1tr, X1v, X1te = train_test_split_data(X1, test_size = test_size, seed=seed)\n",
    "    X2tr, X2v, X2te = train_test_split_data(X2, test_size = test_size, seed=seed)\n",
    "    train_dataset = Data.TensorDataset(X1tr, X2tr)        \n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=len(X1tr) // 3, shuffle=False) \n",
    "    train_loss, last_loss = 0, 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (X1b, X2b, ) in enumerate(train_loader):\n",
    "            loss = model.loss(X1b, X2b)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss\n",
    "        if (epoch % display_intvl == 0 or torch.abs(last_loss - train_loss) < small):\n",
    "            val_loss = model.loss(X1v, X2v)\n",
    "            print('Epoch {} (lr: {:11f}):  Loss: {:.3f} (Val-Loss: {:.3f})'.format(\n",
    "                epoch, scheduler.get_last_lr()[0], train_loss.item(), val_loss.item()))\n",
    "        if torch.abs(last_loss - train_loss) < small:\n",
    "            break\n",
    "        last_loss = train_loss\n",
    "        train_loss = 0\n",
    "        scheduler.step()\n",
    "    print(\"Testing MSE:  \", model.loss(X1te, X2te))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20c12278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "pca = None\n",
    "\n",
    "def pca_transform(df, var_dict, n_component, pca_thres = 0.8):\n",
    "    global pca\n",
    "    pca = PCA(n_components=n_component)\n",
    "    to_ret = pd.DataFrame()\n",
    "    for cat in var_dict['Category'].unique():\n",
    "        if cat != 'DEMO':\n",
    "            var_names = var_dict[var_dict['Category'] == cat]['VariableName'].values\n",
    "            var_names = set(var_names).intersection(set(df.columns))\n",
    "            if len(var_names) > n_component:\n",
    "                pca_transformed = pca.fit_transform(scaler.fit_transform(df.loc[:,var_names]))\n",
    "                num_selected_1 = np.sum(pca.explained_variance_ratio_ > pca_thres)\n",
    "                num_selected_2 = np.argmax(-np.diff(pca.explained_variance_ratio_, n=1) > 0.1) + 1\n",
    "                num_selected = max(num_selected_1, num_selected_2)\n",
    "                temp = pd.DataFrame(pca_transformed[:,:num_selected])\n",
    "                temp.columns = [\"{}_{}\".format(cat, i+1) for i in range(num_selected)]\n",
    "                print(cat, pca.explained_variance_ratio_, num_selected)\n",
    "                to_ret = pd.concat([to_ret, temp], axis=1)\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8607d",
   "metadata": {},
   "source": [
    "## UDS -> UDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ac618dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDF [0.76768548 0.10899955 0.03684562 0.03143016 0.01722622] 1\n",
      "GDS [0.33769396 0.07255963 0.06801303 0.05805296 0.05455118] 1\n",
      "FAS [0.75274188 0.05874551 0.04165319 0.03237024 0.02882413] 1\n",
      "NPI [0.30584755 0.07569686 0.07295629 0.06332827 0.06178203] 1\n",
      "NEURO [0.32660678 0.12672186 0.08952474 0.07479331 0.05000349] 1\n",
      "(44740, 9)\n"
     ]
    }
   ],
   "source": [
    "uds_pca = pca_transform(uds.drop(uds_drop_columns, axis=1), uds_dict, 5, pca_thres=0.2)\n",
    "uds_pca = pd.concat([uds[['SEX', 'NACCAGE', 'EDUC', 'NACCAPOE']].reset_index(drop=True), uds_pca], axis=1)\n",
    "print(uds_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "85f2fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (lr:    0.010000):  Loss: 3.552 (Val-Loss: 0.928)\n",
      "Epoch 20 (lr:    0.007500):  Loss: 1.667 (Val-Loss: 0.543)\n",
      "Epoch 40 (lr:    0.002500):  Loss: 1.518 (Val-Loss: 0.499)\n",
      "Epoch 60 (lr:    0.000000):  Loss: 1.502 (Val-Loss: 0.494)\n",
      "Epoch 80 (lr:    0.002500):  Loss: 1.484 (Val-Loss: 0.488)\n",
      "Epoch 100 (lr:    0.007500):  Loss: 1.440 (Val-Loss: 0.469)\n",
      "Epoch 120 (lr:    0.010000):  Loss: 1.465 (Val-Loss: 0.471)\n",
      "Epoch 140 (lr:    0.007500):  Loss: 1.332 (Val-Loss: 0.442)\n",
      "Epoch 160 (lr:    0.002500):  Loss: 1.288 (Val-Loss: 0.423)\n",
      "Epoch 180 (lr:    0.000000):  Loss: 1.277 (Val-Loss: 0.422)\n",
      "Epoch 200 (lr:    0.002500):  Loss: 1.273 (Val-Loss: 0.420)\n",
      "Epoch 220 (lr:    0.007500):  Loss: 1.262 (Val-Loss: 0.417)\n",
      "Epoch 240 (lr:    0.010000):  Loss: 1.352 (Val-Loss: 0.453)\n",
      "Epoch 260 (lr:    0.007500):  Loss: 1.241 (Val-Loss: 0.408)\n",
      "Epoch 280 (lr:    0.002500):  Loss: 1.217 (Val-Loss: 0.402)\n",
      "Testing MSE:   tensor(0.4021, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "uds_sub = uds.drop(uds_drop_columns, axis=1)\n",
    "uds_scaler = StandardScaler()\n",
    "uds_model = trainAE(uds_scaler.fit_transform(uds_sub), [81, 32, 16, 9], [9,32, 81], \n",
    "                    lr=1e-2, epochs=300, weight_decay=1e-5, T_max=60, small=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0fe2b",
   "metadata": {},
   "source": [
    "## MRI -> MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0d9bdef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (lr:    0.010000):  Loss: 3.003 (Val-Loss: 0.879)\n",
      "Epoch 50 (lr:    0.009990):  Loss: 1.137 (Val-Loss: 0.371)\n",
      "Epoch 100 (lr:    0.009961):  Loss: 1.095 (Val-Loss: 0.373)\n",
      "Epoch 150 (lr:    0.009911):  Loss: 1.077 (Val-Loss: 0.380)\n",
      "Epoch 200 (lr:    0.009843):  Loss: 1.062 (Val-Loss: 0.384)\n",
      "Epoch 250 (lr:    0.009755):  Loss: 1.054 (Val-Loss: 0.390)\n",
      "Epoch 300 (lr:    0.009649):  Loss: 1.044 (Val-Loss: 0.392)\n",
      "Epoch 350 (lr:    0.009524):  Loss: 1.045 (Val-Loss: 0.395)\n",
      "Epoch 400 (lr:    0.009382):  Loss: 1.039 (Val-Loss: 0.398)\n",
      "Epoch 450 (lr:    0.009222):  Loss: 1.031 (Val-Loss: 0.402)\n",
      "Epoch 500 (lr:    0.009045):  Loss: 1.028 (Val-Loss: 0.405)\n",
      "Epoch 550 (lr:    0.008853):  Loss: 1.030 (Val-Loss: 0.407)\n",
      "Epoch 600 (lr:    0.008645):  Loss: 1.026 (Val-Loss: 0.407)\n",
      "Epoch 650 (lr:    0.008423):  Loss: 1.022 (Val-Loss: 0.410)\n",
      "Epoch 700 (lr:    0.008187):  Loss: 1.023 (Val-Loss: 0.412)\n",
      "Epoch 750 (lr:    0.007939):  Loss: 1.022 (Val-Loss: 0.413)\n",
      "Epoch 800 (lr:    0.007679):  Loss: 1.017 (Val-Loss: 0.414)\n",
      "Epoch 850 (lr:    0.007409):  Loss: 1.015 (Val-Loss: 0.415)\n",
      "Epoch 900 (lr:    0.007129):  Loss: 1.013 (Val-Loss: 0.417)\n",
      "Epoch 950 (lr:    0.006841):  Loss: 1.010 (Val-Loss: 0.418)\n",
      "Epoch 1000 (lr:    0.006545):  Loss: 1.009 (Val-Loss: 0.419)\n",
      "Epoch 1050 (lr:    0.006243):  Loss: 1.007 (Val-Loss: 0.422)\n",
      "Epoch 1100 (lr:    0.005937):  Loss: 1.006 (Val-Loss: 0.423)\n",
      "Epoch 1150 (lr:    0.005627):  Loss: 1.004 (Val-Loss: 0.424)\n",
      "Epoch 1200 (lr:    0.005314):  Loss: 1.002 (Val-Loss: 0.425)\n",
      "Epoch 1250 (lr:    0.005000):  Loss: 1.002 (Val-Loss: 0.427)\n",
      "Epoch 1300 (lr:    0.004686):  Loss: 0.999 (Val-Loss: 0.428)\n",
      "Epoch 1350 (lr:    0.004373):  Loss: 0.998 (Val-Loss: 0.429)\n",
      "Epoch 1400 (lr:    0.004063):  Loss: 0.997 (Val-Loss: 0.431)\n",
      "Epoch 1450 (lr:    0.003757):  Loss: 0.997 (Val-Loss: 0.432)\n",
      "Epoch 1500 (lr:    0.003455):  Loss: 0.996 (Val-Loss: 0.432)\n",
      "Epoch 1550 (lr:    0.003159):  Loss: 0.994 (Val-Loss: 0.434)\n",
      "Epoch 1600 (lr:    0.002871):  Loss: 0.993 (Val-Loss: 0.435)\n",
      "Epoch 1650 (lr:    0.002591):  Loss: 0.993 (Val-Loss: 0.436)\n",
      "Epoch 1700 (lr:    0.002321):  Loss: 0.992 (Val-Loss: 0.437)\n",
      "Epoch 1750 (lr:    0.002061):  Loss: 0.992 (Val-Loss: 0.437)\n",
      "Epoch 1800 (lr:    0.001813):  Loss: 0.991 (Val-Loss: 0.438)\n",
      "Epoch 1850 (lr:    0.001577):  Loss: 0.991 (Val-Loss: 0.439)\n",
      "Epoch 1900 (lr:    0.001355):  Loss: 0.990 (Val-Loss: 0.439)\n",
      "Epoch 1950 (lr:    0.001147):  Loss: 0.990 (Val-Loss: 0.440)\n",
      "Epoch 2000 (lr:    0.000955):  Loss: 0.989 (Val-Loss: 0.440)\n",
      "Epoch 2035 (lr:    0.000830):  Loss: 0.989 (Val-Loss: 0.440)\n",
      "Testing MSE:   tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mri_scaler = StandardScaler()\n",
    "mri_sub = mri.drop(mri_drop_columns, axis=1)\n",
    "mri_model = trainAE(mri_scaler.fit_transform(mri_sub), [155,64,16,5], [5,64,155], \n",
    "                    lr=1e-2, epochs=5000, weight_decay=1e-5, T_max=2500, small=1e-7, display_intvl=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96cc981c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regional gray matter volumes [0.56327925 0.17272831 0.06163145 0.04907454 0.0346227 ] 2\n",
      "Regional cortical thicknesses [0.44952865 0.05045401 0.03774226 0.03147718 0.02400231] 1\n",
      "PET scan data [0.37569421 0.12838299 0.07782488 0.0606391  0.02956345] 2\n",
      "(2873, 5)\n"
     ]
    }
   ],
   "source": [
    "mri_pca = pca_transform(mri.drop(mri_drop_columns, axis=1), mri_dict, 5, pca_thres=0.1)\n",
    "print(mri_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664239c",
   "metadata": {},
   "source": [
    "# UDS -> MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ce8c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "uds_mri_scaler, mri_mri_scaler = StandardScaler(), StandardScaler()\n",
    "uds_mri_id = set(uds['NACCID']).intersection(mri['NACCID'])\n",
    "uds_mri_sub = uds[uds['NACCID'].isin(uds_mri_id)].sort_values('NACCID').drop(uds_drop_columns, axis=1)\n",
    "mri_mri_sub = mri[mri['NACCID'].isin(uds_mri_id)].sort_values('NACCID').drop(mri_drop_columns, axis=1)\n",
    "assert(uds_mri_sub.shape[0] == mri_mri_sub.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39874a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (lr:    0.010000):  Loss: 3.729 (Val-Loss: 0.973)\n",
      "Epoch 50 (lr:    0.009993):  Loss: 2.055 (Val-Loss: 0.894)\n",
      "Epoch 100 (lr:    0.009973):  Loss: 1.910 (Val-Loss: 1.012)\n",
      "Epoch 150 (lr:    0.009938):  Loss: 1.820 (Val-Loss: 0.966)\n",
      "Epoch 200 (lr:    0.009891):  Loss: 1.731 (Val-Loss: 1.054)\n",
      "Epoch 250 (lr:    0.009830):  Loss: 1.752 (Val-Loss: 1.026)\n",
      "Epoch 300 (lr:    0.009755):  Loss: 1.676 (Val-Loss: 1.069)\n",
      "Epoch 350 (lr:    0.009668):  Loss: 1.690 (Val-Loss: 1.097)\n",
      "Epoch 400 (lr:    0.009568):  Loss: 1.621 (Val-Loss: 1.161)\n",
      "Epoch 450 (lr:    0.009455):  Loss: 1.703 (Val-Loss: 1.229)\n",
      "Epoch 500 (lr:    0.009330):  Loss: 1.611 (Val-Loss: 1.199)\n",
      "Epoch 550 (lr:    0.009193):  Loss: 1.607 (Val-Loss: 1.122)\n",
      "Epoch 600 (lr:    0.009045):  Loss: 1.597 (Val-Loss: 1.115)\n",
      "Epoch 650 (lr:    0.008886):  Loss: 1.563 (Val-Loss: 1.174)\n",
      "Epoch 700 (lr:    0.008716):  Loss: 1.553 (Val-Loss: 1.170)\n",
      "Epoch 750 (lr:    0.008536):  Loss: 1.665 (Val-Loss: 1.147)\n",
      "Epoch 800 (lr:    0.008346):  Loss: 1.547 (Val-Loss: 1.210)\n",
      "Epoch 850 (lr:    0.008147):  Loss: 1.543 (Val-Loss: 1.210)\n",
      "Epoch 900 (lr:    0.007939):  Loss: 1.533 (Val-Loss: 1.247)\n",
      "Epoch 950 (lr:    0.007723):  Loss: 1.528 (Val-Loss: 1.223)\n",
      "Epoch 1000 (lr:    0.007500):  Loss: 1.507 (Val-Loss: 1.247)\n",
      "Epoch 1050 (lr:    0.007270):  Loss: 1.519 (Val-Loss: 1.212)\n",
      "Epoch 1100 (lr:    0.007034):  Loss: 1.526 (Val-Loss: 1.282)\n",
      "Epoch 1150 (lr:    0.006792):  Loss: 1.514 (Val-Loss: 1.236)\n",
      "Epoch 1200 (lr:    0.006545):  Loss: 1.525 (Val-Loss: 1.247)\n",
      "Epoch 1250 (lr:    0.006294):  Loss: 1.531 (Val-Loss: 1.250)\n",
      "Epoch 1300 (lr:    0.006040):  Loss: 1.496 (Val-Loss: 1.284)\n",
      "Epoch 1350 (lr:    0.005782):  Loss: 1.522 (Val-Loss: 1.244)\n",
      "Epoch 1400 (lr:    0.005523):  Loss: 1.479 (Val-Loss: 1.266)\n",
      "Epoch 1450 (lr:    0.005262):  Loss: 1.475 (Val-Loss: 1.307)\n",
      "Epoch 1500 (lr:    0.005000):  Loss: 1.510 (Val-Loss: 1.310)\n",
      "Epoch 1550 (lr:    0.004738):  Loss: 1.456 (Val-Loss: 1.322)\n",
      "Epoch 1600 (lr:    0.004477):  Loss: 1.454 (Val-Loss: 1.320)\n",
      "Epoch 1650 (lr:    0.004218):  Loss: 1.463 (Val-Loss: 1.340)\n",
      "Epoch 1700 (lr:    0.003960):  Loss: 1.467 (Val-Loss: 1.341)\n",
      "Epoch 1750 (lr:    0.003706):  Loss: 1.453 (Val-Loss: 1.322)\n",
      "Epoch 1800 (lr:    0.003455):  Loss: 1.448 (Val-Loss: 1.336)\n",
      "Epoch 1850 (lr:    0.003208):  Loss: 1.446 (Val-Loss: 1.349)\n",
      "Epoch 1900 (lr:    0.002966):  Loss: 1.438 (Val-Loss: 1.358)\n",
      "Epoch 1950 (lr:    0.002730):  Loss: 1.440 (Val-Loss: 1.341)\n",
      "Epoch 2000 (lr:    0.002500):  Loss: 1.439 (Val-Loss: 1.335)\n",
      "Epoch 2050 (lr:    0.002277):  Loss: 1.434 (Val-Loss: 1.351)\n",
      "Epoch 2100 (lr:    0.002061):  Loss: 1.430 (Val-Loss: 1.355)\n",
      "Epoch 2150 (lr:    0.001853):  Loss: 1.430 (Val-Loss: 1.370)\n",
      "Epoch 2200 (lr:    0.001654):  Loss: 1.427 (Val-Loss: 1.372)\n",
      "Epoch 2250 (lr:    0.001464):  Loss: 1.426 (Val-Loss: 1.367)\n",
      "Epoch 2300 (lr:    0.001284):  Loss: 1.425 (Val-Loss: 1.372)\n",
      "Epoch 2350 (lr:    0.001114):  Loss: 1.425 (Val-Loss: 1.379)\n",
      "Epoch 2400 (lr:    0.000955):  Loss: 1.424 (Val-Loss: 1.379)\n",
      "Epoch 2450 (lr:    0.000807):  Loss: 1.423 (Val-Loss: 1.379)\n",
      "Epoch 2500 (lr:    0.000670):  Loss: 1.422 (Val-Loss: 1.381)\n",
      "Epoch 2550 (lr:    0.000545):  Loss: 1.422 (Val-Loss: 1.383)\n",
      "Epoch 2600 (lr:    0.000432):  Loss: 1.422 (Val-Loss: 1.383)\n",
      "Epoch 2650 (lr:    0.000332):  Loss: 1.421 (Val-Loss: 1.384)\n",
      "Epoch 2698 (lr:    0.000248):  Loss: 1.421 (Val-Loss: 1.385)\n",
      "Testing MSE:   tensor(1.3618, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "uds_mri_model = trainAE(uds_mri_scaler.fit_transform(uds_mri_sub), [81, 32, 16, 5], [5, 32, 64, 155], \n",
    "                        X2=mri_mri_scaler.fit_transform(mri_mri_sub), \n",
    "                        lr=1e-2, epochs=5000, weight_decay=1e-5, T_max=3000, small=1e-9, display_intvl=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eea593",
   "metadata": {},
   "source": [
    "# UDS -> CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "604ee3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uds_csf_scaler, csf_csf_scaler = StandardScaler(), StandardScaler()\n",
    "uds_csf_id = set(uds['NACCID']).intersection(csf['NACCID'])\n",
    "uds_csf_sub = uds[uds['NACCID'].isin(uds_csf_id)].sort_values('NACCID').drop(uds_drop_columns, axis=1)\n",
    "csf_csf_sub = csf[csf['NACCID'].isin(uds_csf_id)].sort_values('NACCID').drop(['NACCID', 'CSFABMD', 'CSFTTMD', 'CSFPTMD'], axis=1)\n",
    "assert(uds_csf_sub.shape[0] == csf_csf_sub.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60b5192b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (lr:    0.010000):  Loss: 3.223 (Val-Loss: 1.158)\n",
      "Epoch 50 (lr:    0.009993):  Loss: 1.050 (Val-Loss: 1.242)\n",
      "Epoch 100 (lr:    0.009973):  Loss: 0.749 (Val-Loss: 1.406)\n",
      "Epoch 150 (lr:    0.009938):  Loss: 0.627 (Val-Loss: 1.559)\n",
      "Epoch 200 (lr:    0.009891):  Loss: 0.562 (Val-Loss: 1.653)\n",
      "Epoch 250 (lr:    0.009830):  Loss: 0.501 (Val-Loss: 1.812)\n",
      "Epoch 300 (lr:    0.009755):  Loss: 0.502 (Val-Loss: 1.875)\n",
      "Epoch 350 (lr:    0.009668):  Loss: 0.481 (Val-Loss: 1.920)\n",
      "Epoch 400 (lr:    0.009568):  Loss: 0.451 (Val-Loss: 2.045)\n",
      "Epoch 450 (lr:    0.009455):  Loss: 0.440 (Val-Loss: 2.079)\n",
      "Epoch 500 (lr:    0.009330):  Loss: 0.439 (Val-Loss: 2.024)\n",
      "Epoch 550 (lr:    0.009193):  Loss: 0.449 (Val-Loss: 2.141)\n",
      "Epoch 600 (lr:    0.009045):  Loss: 0.421 (Val-Loss: 2.044)\n",
      "Epoch 650 (lr:    0.008886):  Loss: 0.414 (Val-Loss: 2.163)\n",
      "Epoch 700 (lr:    0.008716):  Loss: 0.394 (Val-Loss: 2.211)\n",
      "Epoch 750 (lr:    0.008536):  Loss: 0.390 (Val-Loss: 2.163)\n",
      "Epoch 800 (lr:    0.008346):  Loss: 0.387 (Val-Loss: 2.304)\n",
      "Epoch 850 (lr:    0.008147):  Loss: 0.368 (Val-Loss: 2.304)\n",
      "Epoch 900 (lr:    0.007939):  Loss: 0.373 (Val-Loss: 2.300)\n",
      "Epoch 950 (lr:    0.007723):  Loss: 0.364 (Val-Loss: 2.342)\n",
      "Epoch 1000 (lr:    0.007500):  Loss: 0.365 (Val-Loss: 2.228)\n",
      "Epoch 1050 (lr:    0.007270):  Loss: 0.338 (Val-Loss: 2.219)\n",
      "Epoch 1100 (lr:    0.007034):  Loss: 0.355 (Val-Loss: 2.202)\n",
      "Epoch 1150 (lr:    0.006792):  Loss: 0.329 (Val-Loss: 2.272)\n",
      "Epoch 1200 (lr:    0.006545):  Loss: 0.340 (Val-Loss: 2.188)\n",
      "Epoch 1250 (lr:    0.006294):  Loss: 0.328 (Val-Loss: 2.206)\n",
      "Epoch 1300 (lr:    0.006040):  Loss: 0.335 (Val-Loss: 2.264)\n",
      "Epoch 1350 (lr:    0.005782):  Loss: 0.318 (Val-Loss: 2.235)\n",
      "Epoch 1400 (lr:    0.005523):  Loss: 0.318 (Val-Loss: 2.284)\n",
      "Epoch 1450 (lr:    0.005262):  Loss: 0.309 (Val-Loss: 2.233)\n",
      "Epoch 1500 (lr:    0.005000):  Loss: 0.312 (Val-Loss: 2.280)\n",
      "Epoch 1550 (lr:    0.004738):  Loss: 0.309 (Val-Loss: 2.328)\n",
      "Epoch 1600 (lr:    0.004477):  Loss: 0.303 (Val-Loss: 2.329)\n",
      "Epoch 1650 (lr:    0.004218):  Loss: 0.302 (Val-Loss: 2.347)\n",
      "Epoch 1700 (lr:    0.003960):  Loss: 0.300 (Val-Loss: 2.288)\n",
      "Epoch 1750 (lr:    0.003706):  Loss: 0.297 (Val-Loss: 2.350)\n",
      "Epoch 1800 (lr:    0.003455):  Loss: 0.293 (Val-Loss: 2.299)\n",
      "Epoch 1850 (lr:    0.003208):  Loss: 0.293 (Val-Loss: 2.319)\n",
      "Epoch 1900 (lr:    0.002966):  Loss: 0.292 (Val-Loss: 2.339)\n",
      "Epoch 1950 (lr:    0.002730):  Loss: 0.285 (Val-Loss: 2.324)\n",
      "Epoch 2000 (lr:    0.002500):  Loss: 0.282 (Val-Loss: 2.352)\n",
      "Epoch 2050 (lr:    0.002277):  Loss: 0.285 (Val-Loss: 2.349)\n",
      "Epoch 2100 (lr:    0.002061):  Loss: 0.281 (Val-Loss: 2.362)\n",
      "Epoch 2150 (lr:    0.001853):  Loss: 0.278 (Val-Loss: 2.380)\n",
      "Epoch 2200 (lr:    0.001654):  Loss: 0.276 (Val-Loss: 2.389)\n",
      "Epoch 2250 (lr:    0.001464):  Loss: 0.275 (Val-Loss: 2.386)\n",
      "Epoch 2300 (lr:    0.001284):  Loss: 0.274 (Val-Loss: 2.390)\n",
      "Epoch 2350 (lr:    0.001114):  Loss: 0.272 (Val-Loss: 2.387)\n",
      "Epoch 2400 (lr:    0.000955):  Loss: 0.272 (Val-Loss: 2.396)\n",
      "Epoch 2450 (lr:    0.000807):  Loss: 0.271 (Val-Loss: 2.394)\n",
      "Epoch 2500 (lr:    0.000670):  Loss: 0.271 (Val-Loss: 2.393)\n",
      "Epoch 2550 (lr:    0.000545):  Loss: 0.270 (Val-Loss: 2.396)\n",
      "Epoch 2600 (lr:    0.000432):  Loss: 0.269 (Val-Loss: 2.402)\n",
      "Epoch 2650 (lr:    0.000332):  Loss: 0.269 (Val-Loss: 2.403)\n",
      "Epoch 2700 (lr:    0.000245):  Loss: 0.269 (Val-Loss: 2.405)\n",
      "Epoch 2750 (lr:    0.000170):  Loss: 0.268 (Val-Loss: 2.405)\n",
      "Epoch 2800 (lr:    0.000109):  Loss: 0.268 (Val-Loss: 2.405)\n",
      "Epoch 2850 (lr:    0.000062):  Loss: 0.268 (Val-Loss: 2.405)\n",
      "Epoch 2900 (lr:    0.000027):  Loss: 0.268 (Val-Loss: 2.406)\n",
      "Epoch 2950 (lr:    0.000007):  Loss: 0.268 (Val-Loss: 2.406)\n",
      "Epoch 2994 (lr:    0.000000):  Loss: 0.268 (Val-Loss: 2.406)\n",
      "Testing MSE:   tensor(2.8489, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "uds_csf_model = trainAE(uds_csf_scaler.fit_transform(uds_csf_sub), [81, 32, 16, 3], [3, 8, 3], \n",
    "                        X2=csf_csf_scaler.fit_transform(csf_csf_sub), \n",
    "                        lr=1e-2, epochs=5000, weight_decay=1e-5, T_max=3000, small=1e-9, display_intvl=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52ff4e",
   "metadata": {},
   "source": [
    "# Construct Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c7f8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44740, 10)\n"
     ]
    }
   ],
   "source": [
    "Z_uds = uds_model.encode(torch.tensor(uds_scaler.fit_transform(uds_sub)).float().to(device)).detach().cpu().numpy()\n",
    "uds_new = pd.DataFrame(Z_uds)\n",
    "uds_new['NACCID'] = uds['NACCID'].values\n",
    "print(uds_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a7e32d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44740, 7)\n"
     ]
    }
   ],
   "source": [
    "Z_mri = mri_model.encode(torch.tensor(mri_scaler.fit_transform(mri_sub)).float().to(device)).detach().cpu().numpy()\n",
    "mri_new = pd.DataFrame(Z_mri)\n",
    "mri_new['NACCID'] = mri['NACCID'].values\n",
    "mri_new['MRI-imputed'] = 0\n",
    "\n",
    "uds_missing_mri_sub = uds[~uds['NACCID'].isin(uds_mri_id)]\n",
    "\n",
    "Z_uds_mri = uds_mri_model.encode(torch.tensor(\n",
    "    uds_mri_scaler.fit_transform(uds_missing_mri_sub.drop(uds_drop_columns, axis=1))\n",
    ").float().to(device)).detach().cpu().numpy()\n",
    "missing_mri = pd.DataFrame(Z_uds_mri)\n",
    "missing_mri['NACCID'] = uds_missing_mri_sub['NACCID'].values\n",
    "missing_mri['MRI-imputed'] = 1\n",
    "\n",
    "mri_new = pd.concat([mri_new, missing_mri])\n",
    "mri_new = mri_new[mri_new['NACCID'].isin(uds_new['NACCID'])]\n",
    "print(mri_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "77ba921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44748, 5)\n"
     ]
    }
   ],
   "source": [
    "csf_new = csf.drop(['CSFABMD', 'CSFTTMD', 'CSFPTMD'], axis=1)\n",
    "csf_new[['CSFABETA', 'CSFTTAU', 'CSFPTAU']] = csf_csf_scaler.transform(csf_new[['CSFABETA', 'CSFTTAU', 'CSFPTAU']])\n",
    "\n",
    "uds_missing_csf_sub = uds[~uds['NACCID'].isin(uds_csf_id)]\n",
    "Z_uds_csf = uds_csf_model.encode(torch.tensor(\n",
    "    uds_csf_scaler.fit_transform(uds_missing_csf_sub.drop(uds_drop_columns, axis=1))\n",
    ").float().to(device)).detach().cpu().numpy()\n",
    "missing_csf = pd.DataFrame(Z_uds_csf, columns=csf_new.columns[1:])\n",
    "missing_csf['NACCID'] = uds_missing_csf_sub['NACCID'].values\n",
    "missing_csf['csf-imputed'] = 1\n",
    "\n",
    "csf_new['csf-imputed'] = 0\n",
    "csf_new = pd.concat([csf_new, missing_csf], axis=0)\n",
    "print(csf_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fa1c9",
   "metadata": {},
   "source": [
    "# Fit Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0540ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_summary(clf, Xtr, ytr, Xte, yte, confusion_metrix = False):\n",
    "    if confusion_metrix:\n",
    "        print(metrics.confusion_matrix(ytr, clf.predict(Xtr)))\n",
    "        print(metrics.confusion_matrix(yte, clf.predict(Xte)))\n",
    "    acctr = metrics.accuracy_score(ytr, clf.predict(Xtr))\n",
    "    auctr = metrics.roc_auc_score(ytr, clf.predict_proba(Xtr), average='macro', multi_class='ovo')\n",
    "    f1tr_macro = metrics.f1_score(ytr, clf.predict(Xtr), average='macro')\n",
    "    f1tr_micro = metrics.f1_score(ytr, clf.predict(Xtr), average='micro')\n",
    "    \n",
    "    accte = metrics.accuracy_score(yte, clf.predict(Xte))\n",
    "    aucte = metrics.roc_auc_score(yte, clf.predict_proba(Xte), average='macro', multi_class='ovo')\n",
    "    f1te_macro = metrics.f1_score(yte, clf.predict(Xte), average='macro')\n",
    "    f1te_micro = metrics.f1_score(yte, clf.predict(Xte), average='micro')\n",
    " \n",
    "    metrics_df = pd.DataFrame.from_dict({\"Train\": {\"Acc\": acctr, \"AUC\": auctr, \"F1-macro\": f1tr_macro, \"F1-micro\": f1tr_micro}, \n",
    "                                         \"Test\": {\"Acc\": accte, \"AUC\": aucte, \"F1-macro\": f1te_macro, \"F1-micro\": f1te_micro}}, \n",
    "                                         orient='Index')\n",
    "    return metrics_df.round(3)\n",
    "\n",
    "def train_logistic(df, target, C=0.4, l1_ratio=0.3, df_name=None):\n",
    "    clf = LogisticRegression(random_state=48, max_iter=1500, solver='saga', penalty='elasticnet', C=C, l1_ratio=l1_ratio)\n",
    "    class_n = df.groupby(target)['NACCID'].count()\n",
    "    df = df.loc[df[target].isin(class_n[class_n>=5].index), ]\n",
    "    if df_name is not None and len(class_n) != len(class_n[class_n>5].index):\n",
    "        print(\"{}: {} label(s) have samples size less than 5\".format(df_name, len(class_n[class_n<5])))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['NACCID', target], axis=1), \n",
    "                                                        df[target], stratify=df[target], test_size = 0.3, random_state=48)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    return print_summary(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd998f6c",
   "metadata": {},
   "source": [
    "## UDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30db5b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>F1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Acc    AUC  F1-macro  F1-micro\n",
       "Train  0.875  0.940     0.776     0.875\n",
       "Test   0.876  0.939     0.778     0.876"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = uds_new.merge(uds[['NACCID', 'NACCAD3']], on='NACCID', how='inner')\n",
    "train_logistic(X, 'NACCAD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1177",
   "metadata": {},
   "source": [
    "## UDS + MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "75c64941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>F1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.877</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.878</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Acc    AUC  F1-macro  F1-micro\n",
       "Train  0.877  0.941     0.779     0.877\n",
       "Test   0.878  0.940     0.782     0.878"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = uds_new.merge(uds[['NACCID', 'NACCAD3']], on='NACCID', how='inner')\n",
    "X = X.merge(mri_new, on='NACCID', how='inner')\n",
    "train_logistic(X, 'NACCAD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c0c70",
   "metadata": {},
   "source": [
    "## UDS + MRI + CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a0af42ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>F1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.881</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.881</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Acc    AUC  F1-macro  F1-micro\n",
       "Train  0.881  0.945     0.789     0.881\n",
       "Test   0.881  0.945     0.789     0.881"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = uds_new.merge(uds[['NACCID', 'NACCAD3']], on='NACCID', how='inner')\n",
    "X = X.merge(mri_new, on='NACCID', how='inner')\n",
    "X = X.merge(csf_new, on='NACCID', how='inner')\n",
    "train_logistic(X, 'NACCAD3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
