{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b821f5",
   "metadata": {},
   "source": [
    "# Trying to do fusion using a deep learning method \n",
    "\n",
    "#### importing data + choosing imputing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbf6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.utils.data as Data\n",
    "from torch.nn import functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import pydot\n",
    "import tensorflow as tf \n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Keras specific\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbcc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)#as reproducibility docs\n",
    "    torch.manual_seed(seed)# as reproducibility docs\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False# as reproducibility docs\n",
    "    torch.backends.cudnn.deterministic = True# as reproducibility docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ecf33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44740, 89) (2873, 161) (2180, 7)\n"
     ]
    }
   ],
   "source": [
    "def load_data(impute_method = 'RF'):\n",
    "    uds = pd.read_csv(\"../data/data_imputed/{}/uds.csv\".format(impute_method))\n",
    "    uds['datetime'] = pd.to_datetime(uds['datetime'])\n",
    "    uds = uds.dropna(subset=['EDUC'])\n",
    "    \n",
    "    mri = pd.read_csv(\"../data/data_imputed/{}/mri.csv\".format(impute_method))\n",
    "    mri['datetime'] = pd.to_datetime(mri['datetime'])\n",
    "    \n",
    "    csf = pd.read_csv(\"../data/data_imputed/{}/csf.csv\".format(impute_method))\n",
    "    return uds, mri, csf\n",
    "\n",
    "uds_dict = pd.read_csv(\"../data/data_dictionary/uds_feature_dictionary_cleaned.csv\")\n",
    "mri_dict = pd.read_csv(\"../data/data_dictionary/mri_feature_dictionary_cleaned.csv\") \n",
    "\n",
    "uds_drop_columns = ['NACCID', 'NACCADC', 'NACCVNUM', 'datetime', 'NACCUDSD', 'NACCALZP', 'NACCAD3', 'NACCAD5']\n",
    "mri_drop_columns = ['NACCID', 'NACCVNUM', 'datetime', 'datetime_UDS', 'timediff', 'within-a-year']\n",
    "csf_drop_columns = ['NACCID', 'CSFABMD', 'CSFTTMD', 'CSFPTMD']\n",
    "\n",
    "uds, mri, csf = load_data()\n",
    "print(uds.shape, mri.shape, csf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a781409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NACCID</th>\n",
       "      <th>NACCAD5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NACC020208</td>\n",
       "      <td>MCI-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NACC107305</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NACC151065</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NACC187327</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NACC188799</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45095</th>\n",
       "      <td>NACC993286</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45096</th>\n",
       "      <td>NACC994463</td>\n",
       "      <td>Dementia-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45097</th>\n",
       "      <td>NACC995870</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45098</th>\n",
       "      <td>NACC998475</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45099</th>\n",
       "      <td>NACC999391</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44740 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NACCID      NACCAD5\n",
       "0      NACC020208       MCI-AD\n",
       "1      NACC107305      Healthy\n",
       "2      NACC151065    MCI-NonAD\n",
       "3      NACC187327      Healthy\n",
       "4      NACC188799    MCI-NonAD\n",
       "...           ...          ...\n",
       "45095  NACC993286          NaN\n",
       "45096  NACC994463  Dementia-AD\n",
       "45097  NACC995870      Healthy\n",
       "45098  NACC998475    MCI-NonAD\n",
       "45099  NACC999391    MCI-NonAD\n",
       "\n",
       "[44740 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uds[['NACCID','NACCAD5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03df1e",
   "metadata": {},
   "source": [
    "#### add classifying variable (UDS/ALZP) to the MRI and CSF data sets\n",
    "\n",
    "Need to add the class for each of the people in each data set so that we can do the initial step of the deep neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fdef745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NACCID</th>\n",
       "      <th>NACCVNUM</th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime_UDS</th>\n",
       "      <th>timediff</th>\n",
       "      <th>within-a-year</th>\n",
       "      <th>NACCICV</th>\n",
       "      <th>NACCBRNV</th>\n",
       "      <th>NACCWMVL</th>\n",
       "      <th>CSFVOL</th>\n",
       "      <th>...</th>\n",
       "      <th>RSUPFRM</th>\n",
       "      <th>RSUPPAR</th>\n",
       "      <th>RSUPPARM</th>\n",
       "      <th>RSUPTEM</th>\n",
       "      <th>RSUPTEMM</th>\n",
       "      <th>RSUPMAR</th>\n",
       "      <th>RSUPMARM</th>\n",
       "      <th>RTRTEM</th>\n",
       "      <th>RTRTEMM</th>\n",
       "      <th>NACCAD5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NACC914950</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>2006-10-31</td>\n",
       "      <td>3775</td>\n",
       "      <td>False</td>\n",
       "      <td>1535.13000</td>\n",
       "      <td>1081.63</td>\n",
       "      <td>504.80000</td>\n",
       "      <td>407.37</td>\n",
       "      <td>...</td>\n",
       "      <td>2.11</td>\n",
       "      <td>10.53</td>\n",
       "      <td>1.61</td>\n",
       "      <td>15.71</td>\n",
       "      <td>2.02</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1.2100</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NACC388999</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>2006-02-21</td>\n",
       "      <td>3776</td>\n",
       "      <td>False</td>\n",
       "      <td>1314.57000</td>\n",
       "      <td>1001.09</td>\n",
       "      <td>437.70000</td>\n",
       "      <td>312.44</td>\n",
       "      <td>...</td>\n",
       "      <td>2.70</td>\n",
       "      <td>10.17</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.07</td>\n",
       "      <td>2.17</td>\n",
       "      <td>8.92</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.6100</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NACC550785</td>\n",
       "      <td>10</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>2006-03-28</td>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "      <td>1571.92000</td>\n",
       "      <td>1210.39</td>\n",
       "      <td>516.57000</td>\n",
       "      <td>358.48</td>\n",
       "      <td>...</td>\n",
       "      <td>2.47</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.64</td>\n",
       "      <td>13.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>10.37</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NACC321645</td>\n",
       "      <td>9</td>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>3621</td>\n",
       "      <td>False</td>\n",
       "      <td>1417.97000</td>\n",
       "      <td>1043.73</td>\n",
       "      <td>431.46000</td>\n",
       "      <td>372.57</td>\n",
       "      <td>...</td>\n",
       "      <td>2.33</td>\n",
       "      <td>12.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>14.07</td>\n",
       "      <td>2.06</td>\n",
       "      <td>8.38</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.5800</td>\n",
       "      <td>MCI-NonAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NACC129206</td>\n",
       "      <td>10</td>\n",
       "      <td>2015-12-09</td>\n",
       "      <td>2006-04-18</td>\n",
       "      <td>3522</td>\n",
       "      <td>False</td>\n",
       "      <td>1553.60000</td>\n",
       "      <td>1086.93</td>\n",
       "      <td>425.40000</td>\n",
       "      <td>464.06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.47</td>\n",
       "      <td>13.44</td>\n",
       "      <td>1.62</td>\n",
       "      <td>16.53</td>\n",
       "      <td>2.32</td>\n",
       "      <td>10.38</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.534</td>\n",
       "      <td>2.2308</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>NACC159647</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-11</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "      <td>1336.42000</td>\n",
       "      <td>981.58</td>\n",
       "      <td>444.05000</td>\n",
       "      <td>351.36</td>\n",
       "      <td>...</td>\n",
       "      <td>2.26</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1.53</td>\n",
       "      <td>11.88</td>\n",
       "      <td>2.24</td>\n",
       "      <td>8.98</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.790</td>\n",
       "      <td>1.5600</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>NACC732291</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>69</td>\n",
       "      <td>True</td>\n",
       "      <td>1339.20000</td>\n",
       "      <td>959.04</td>\n",
       "      <td>419.74000</td>\n",
       "      <td>378.69</td>\n",
       "      <td>...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1.52</td>\n",
       "      <td>11.57</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6.99</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>NACC650247</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>1343.77000</td>\n",
       "      <td>934.67</td>\n",
       "      <td>394.78000</td>\n",
       "      <td>398.83</td>\n",
       "      <td>...</td>\n",
       "      <td>2.39</td>\n",
       "      <td>8.37</td>\n",
       "      <td>1.33</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2.36</td>\n",
       "      <td>6.78</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.5700</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>NACC050273</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>1437.06000</td>\n",
       "      <td>1058.00</td>\n",
       "      <td>435.41000</td>\n",
       "      <td>376.57</td>\n",
       "      <td>...</td>\n",
       "      <td>2.36</td>\n",
       "      <td>7.40</td>\n",
       "      <td>1.30</td>\n",
       "      <td>15.12</td>\n",
       "      <td>2.33</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.850</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>NACC635044</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>2008-01-15</td>\n",
       "      <td>2285</td>\n",
       "      <td>False</td>\n",
       "      <td>1343.14066</td>\n",
       "      <td>1080.86</td>\n",
       "      <td>430.57896</td>\n",
       "      <td>320.46</td>\n",
       "      <td>...</td>\n",
       "      <td>2.51</td>\n",
       "      <td>10.89</td>\n",
       "      <td>1.78</td>\n",
       "      <td>14.53</td>\n",
       "      <td>2.25</td>\n",
       "      <td>10.60</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.860</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows Ã— 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          NACCID  NACCVNUM   datetime datetime_UDS  timediff  within-a-year  \\\n",
       "0     NACC914950        11 2017-03-02   2006-10-31      3775          False   \n",
       "1     NACC388999        11 2016-06-24   2006-02-21      3776          False   \n",
       "2     NACC550785        10 2015-06-02   2006-03-28      3353          False   \n",
       "3     NACC321645         9 2015-12-03   2006-01-03      3621          False   \n",
       "4     NACC129206        10 2015-12-09   2006-04-18      3522          False   \n",
       "...          ...       ...        ...          ...       ...            ...   \n",
       "2855  NACC159647         1 2016-05-11   2016-02-18        83           True   \n",
       "2856  NACC732291         1 2016-06-13   2016-04-05        69           True   \n",
       "2857  NACC650247         1 2016-05-06   2016-04-07        29           True   \n",
       "2858  NACC050273         1 2016-05-24   2016-04-12        42           True   \n",
       "2859  NACC635044         5 2014-04-18   2008-01-15      2285          False   \n",
       "\n",
       "         NACCICV  NACCBRNV   NACCWMVL  CSFVOL  ...  RSUPFRM  RSUPPAR  \\\n",
       "0     1535.13000   1081.63  504.80000  407.37  ...     2.11    10.53   \n",
       "1     1314.57000   1001.09  437.70000  312.44  ...     2.70    10.17   \n",
       "2     1571.92000   1210.39  516.57000  358.48  ...     2.47    13.20   \n",
       "3     1417.97000   1043.73  431.46000  372.57  ...     2.33    12.02   \n",
       "4     1553.60000   1086.93  425.40000  464.06  ...     2.47    13.44   \n",
       "...          ...       ...        ...     ...  ...      ...      ...   \n",
       "2855  1336.42000    981.58  444.05000  351.36  ...     2.26     8.84   \n",
       "2856  1339.20000    959.04  419.74000  378.69  ...     2.65     9.04   \n",
       "2857  1343.77000    934.67  394.78000  398.83  ...     2.39     8.37   \n",
       "2858  1437.06000   1058.00  435.41000  376.57  ...     2.36     7.40   \n",
       "2859  1343.14066   1080.86  430.57896  320.46  ...     2.51    10.89   \n",
       "\n",
       "      RSUPPARM  RSUPTEM  RSUPTEMM  RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  \\\n",
       "0         1.61    15.71      2.02     7.24      1.89   0.720   1.2100   \n",
       "1         2.00    13.07      2.17     8.92      2.09   0.630   1.6100   \n",
       "2         1.64    13.90      2.01    10.37      1.89   0.750   1.9000   \n",
       "3         1.68    14.07      2.06     8.38      1.97   0.980   1.5800   \n",
       "4         1.62    16.53      2.32    10.38      2.02   1.534   2.2308   \n",
       "...        ...      ...       ...      ...       ...     ...      ...   \n",
       "2855      1.53    11.88      2.24     8.98      1.86   0.790   1.5600   \n",
       "2856      1.52    11.57      1.95     6.99      2.07   0.910   1.7000   \n",
       "2857      1.33    12.00      2.36     6.78      1.70   0.710   1.5700   \n",
       "2858      1.30    15.12      2.33     9.02      1.84   0.850   1.2500   \n",
       "2859      1.78    14.53      2.25    10.60      2.03   0.860   1.9000   \n",
       "\n",
       "        NACCAD5  \n",
       "0       Healthy  \n",
       "1     MCI-NonAD  \n",
       "2       Healthy  \n",
       "3     MCI-NonAD  \n",
       "4       Healthy  \n",
       "...         ...  \n",
       "2855    Healthy  \n",
       "2856    Healthy  \n",
       "2857    Healthy  \n",
       "2858    Healthy  \n",
       "2859    Healthy  \n",
       "\n",
       "[2860 rows x 162 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uds_sub_mri = uds[uds['NACCID'].isin(mri['NACCID'])]\n",
    "#uds_sub_mri\n",
    "mri = pd.merge(mri, uds[[\"NACCID\",'NACCAD5']], on=\"NACCID\", how=\"inner\")\n",
    "mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b6d142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Healthy           1563\n",
       "Dementia-AD        469\n",
       "MCI-NonAD          330\n",
       "MCI-AD             306\n",
       "Dementia-NonAD      89\n",
       "Name: NACCAD5, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mri['NACCAD5'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3822e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Healthy           1074\n",
       "Dementia-AD        600\n",
       "MCI-NonAD          168\n",
       "MCI-AD             144\n",
       "Dementia-NonAD      97\n",
       "Name: NACCAD5, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csf = pd.merge(csf, uds[[\"NACCID\",'NACCAD5']], on=\"NACCID\", how=\"inner\")\n",
    "csf['NACCAD5'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e6d4",
   "metadata": {},
   "source": [
    "## first layer - individual DNNs \n",
    "\n",
    "### UDS data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a09bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42753, 90)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recode the response variable to numeric\n",
    "uds['NACCAD5_num'] = uds['NACCAD5'].map({'Healthy':0, 'MCI-NonAD':1, 'MCI-AD':2, 'Dementia-NonAD':3, 'Dementia-AD':4})\n",
    "\n",
    "#get rid of rows where classification has not been possible \n",
    "uds = uds.dropna(subset=['NACCAD5'])\n",
    "\n",
    "#designate which column we are trying to predict\n",
    "target_column = ['NACCAD5_num'] \n",
    "\n",
    "predictors = list(set(list(uds.columns))-set(target_column)- set(uds_drop_columns)-set(['NACCAD5']))\n",
    "uds[predictors] = uds[predictors]/uds[predictors].max()\n",
    "uds.describe()\n",
    "uds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "584779f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['NACC020208'],\n",
       "       ['NACC107305'],\n",
       "       ['NACC151065'],\n",
       "       ...,\n",
       "       ['NACC995870'],\n",
       "       ['NACC998475'],\n",
       "       ['NACC999391']], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictors\n",
    "uds[['NACCID']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "118fcf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40615, 82)\n",
      "(40615, 5)\n"
     ]
    }
   ],
   "source": [
    "X_uds = np.concatenate((uds[['NACCID']].values,uds[predictors].values), axis = 1 )\n",
    "y_uds = uds[target_column].values\n",
    "\n",
    "#change it so that we have an ecoded variable for our classification\n",
    "# one hot encode outputs \n",
    "y_uds = keras.utils.to_categorical(y_uds, num_classes=5)\n",
    "\n",
    "X_uds_train, X_uds_test, y_uds_train, y_uds_test = train_test_split(X_uds, y_uds, test_size=0.05,  random_state=20)\n",
    "print(X_uds_train.shape); print(y_uds_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ddf61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want a record of which IDs were used in training and testing so can keep the right ones when \n",
    "#we merge with output from other models \n",
    "#split X_train into a vector of everything but IDs and the IDs \n",
    "X_uds_train_ID = X_uds_train[:,0]\n",
    "X_uds_train = np.delete(X_uds_train,0, 1 )\n",
    "X_uds_train = X_uds_train.astype(float)\n",
    "X_uds_test_ID = X_uds_test[:,0]\n",
    "X_uds_test = np.delete(X_uds_test,0, 1 )\n",
    "X_uds_test = X_uds_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ddfb242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "count_classes = y_uds_test.shape[1]\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbfe2341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['NACC020208', 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       "       ['NACC107305', 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       "       ['NACC151065', 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       ['NACC995870', 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       "       ['NACC998475', 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       "       ['NACC999391', 0.0, 1.0, 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also need training data with the ID in so that we can merge it later on \n",
    "y_uds_ID = np.column_stack((X_uds[:,0], y_uds))\n",
    "y_uds_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d215bf",
   "metadata": {},
   "source": [
    "### UDS Model set up / compile / run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca9f931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ma = tfa.metrics.F1Score(num_classes=3, average='macro')\n",
    "f1_mi = tfa.metrics.FBetaScore(num_classes=3, average='micro') #this seems to be the same as \n",
    "#the accuracy so i think somehting is wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f71047fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uds = Sequential()\n",
    "model_uds.add(Dense(500, activation='tanh', input_dim=X_uds_train.shape[1]))\n",
    "model_uds.add(Dense(100, activation='tanh'))\n",
    "model_uds.add(Dense(50, activation='sigmoid'))\n",
    "model_uds.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_uds.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', f1_ma])\n",
    "#f1_score is macro F1\n",
    "#fbeta_score is micro F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31bee034",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py\", line 160, in update_state  *\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 3 and 5. Shapes are [3] and [5]. for '{{node AssignAddVariableOp_4}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_4/resource, Sum_3)' with input shapes: [], [5].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23624/2849327981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_uds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_uds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_uds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#pred_train_uds= model_uds.predict(X_uds_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#scores_uds = model_uds.evaluate(X_uds_train, y_uds_train, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print('UDS Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_uds[1], 1 - scores_uds[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py\u001b[0m in \u001b[0;36mtf__update_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     55\u001b[0m                             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval__1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_return_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_weighted_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfalse_positives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_weighted_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfalse_negatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_weighted_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py\", line 160, in update_state  *\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 3 and 5. Shapes are [3] and [5]. for '{{node AssignAddVariableOp_4}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_4/resource, Sum_3)' with input shapes: [], [5].\n"
     ]
    }
   ],
   "source": [
    "model_uds.fit(X_uds_train, y_uds_train, epochs=1)\n",
    "\n",
    "#pred_train_uds= model_uds.predict(X_uds_train)\n",
    "#scores_uds = model_uds.evaluate(X_uds_train, y_uds_train, verbose=0)\n",
    "#print('UDS Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_uds[1], 1 - scores_uds[1]))   \n",
    " \n",
    "pred_test= model_uds.predict(X_uds_test)\n",
    "scores2_uds = model_uds.evaluate(X_uds_test, y_uds_test, verbose=0)\n",
    "print('UDS Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_uds[1], 1 - scores2_uds[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad673de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2529263198375702,\n",
       " 0.8954171538352966,\n",
       " 0.8229899406433105,\n",
       " 0.8954171538352966]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2_uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33daca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "uds_all_output = model_uds.predict(np.delete(X_uds,0, 1 ).astype(float))\n",
    "uds_all_output_ID = np.column_stack((X_uds[:,0], uds_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bba3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbd8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax classification on the full (ish) UDS data set\n",
    "#pred_train_uds_IDs =  np.column_stack((X_uds_train_ID, pred_train_uds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uds_all_ouput_ID.shape) ; print(y_uds_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76369a",
   "metadata": {},
   "source": [
    "## MRI data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the response variable to numeric\n",
    "mri['NACCAD5_num'] = mri['NACCAD5'].map({'Healthy': 0, 'MCI-NonAD': 1 , 'MCI-AD': , 'Dementia-AD':4, 'Dementia-AD':5})\n",
    "\n",
    "#get rid of rows where classification has not been possible \n",
    "mri = mri.dropna(subset=['NACCAD5'])\n",
    "\n",
    "#designate which column we are trying to predict\n",
    "target_column = ['NACCAD5_num'] \n",
    "\n",
    "predictors = list(set(list(mri.columns))-set(target_column)- set(mri_drop_columns)-set(['NACCAD5']))\n",
    "mri[predictors] = mri[predictors]/mri[predictors].max()\n",
    "mri.describe()\n",
    "mri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mri =np.concatenate((mri[['NACCID']].values,mri[predictors].values), axis = 1 )\n",
    "y_mri = mri[target_column].values\n",
    "\n",
    "#change it so that we have an ecoded variable for our classification\n",
    "# one hot encode outputs \n",
    "y_mri = keras.utils.to_categorical(y_mri)\n",
    "\n",
    "X_mri_train, X_mri_test, y_mri_train, y_mri_test = train_test_split(X_mri, y_mri, test_size=0.05,  random_state=20)\n",
    "print(X_mri_train.shape); print(X_mri_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want a record of which IDs were used in training and testing so can keep the right ones when \n",
    "#we merge with output from other models \n",
    "#split X_train into a vector of everything but IDs and the IDs \n",
    "X_mri_train_ID = X_mri_train[:,0]\n",
    "X_mri_train = np.delete(X_mri_train,0, 1 )\n",
    "X_mri_train = X_mri_train.astype(float)\n",
    "X_mri_test_ID = X_mri_test[:,0]\n",
    "X_mri_test = np.delete(X_mri_test,0, 1 )\n",
    "X_mri_test = X_mri_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72168ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also need training data with the ID in so that we can merge it later on \n",
    "y_mri_ID = np.column_stack((X_mri[:,0], y_mri))\n",
    "y_mri_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mri_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1612bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = y_mri_test.shape[1]\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6bb2a4",
   "metadata": {},
   "source": [
    "### MRI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mri = Sequential()\n",
    "model_mri.add(Dense(500, activation='tanh', input_dim=X_mri_train.shape[1]))\n",
    "model_mri.add(Dense(250, activation='tanh'))\n",
    "model_mri.add(Dense(100, activation='tanh'))\n",
    "model_mri.add(Dense(50, activation='tanh'))\n",
    "model_mri.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_mri.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mri.fit(X_mri_train, y_mri_train, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24491093",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_mri= model_mri.predict(X_mri_train)\n",
    "scores_mri = model_mri.evaluate(X_mri_train, y_mri_train, verbose=0)\n",
    "print('MRI Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_mri[1], 1 - scores_mri[1]))   \n",
    " \n",
    "pred_test_mri= model_mri.predict(X_mri_test)\n",
    "scores2_mri = model_mri.evaluate(X_mri_test, y_mri_test, verbose=0)\n",
    "print('MRI Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_mri[1], 1 - scores2_mri[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01936de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "mri_all_output = model_mri.predict(np.delete(X_mri,0, 1 ).astype(float))\n",
    "mri_all_output_ID = np.column_stack((X_mri[:,0], mri_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_all_output_ID.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee85e4",
   "metadata": {},
   "source": [
    "### CSF data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the response variable to numeric\n",
    "csf['NACCAD5_num'] = csf['NACCAD5'].map({'Healthy': 0, 'MCI-NonAD': 1 , 'MCI-AD': , 'Dementia-AD':4, 'Dementia-AD':5})\n",
    "\n",
    "#get rid of rows where classification has not been possible \n",
    "csf = csf.dropna(subset=['NACCAD5'])\n",
    "\n",
    "#designate which column we are trying to predict\n",
    "target_column = ['NACCAD5_num'] \n",
    "\n",
    "predictors = list(set(list(csf.columns))-set(target_column)- set(csf_drop_columns)-set(['NACCAD5']))\n",
    "csf[predictors] = csf[predictors]/csf[predictors].max()\n",
    "#csf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c086ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_csf =  np.concatenate((csf[['NACCID']].values,csf[predictors].values), axis = 1 )\n",
    "y_csf = csf[target_column].values\n",
    "\n",
    "#change it so that we have an ecoded variable for our classification\n",
    "# one hot encode outputs\n",
    "y_csf = keras.utils.to_categorical(y_csf)\n",
    "\n",
    "X_csf_train, X_csf_test, y_csf_train, y_csf_test = train_test_split(X_csf, y_csf, test_size=0.05,  random_state=20)\n",
    "print(X_csf_train.shape); print(X_csf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e934f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = y_csf_test.shape[1]\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482507b6",
   "metadata": {},
   "source": [
    "### CSF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53349d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want a record of which IDs were used in training and testing so can keep the right ones when \n",
    "#we merge with output from other models \n",
    "#split X_train into a vector of everything but IDs and the IDs \n",
    "X_csf_train_ID = X_csf_train[:,0]\n",
    "X_csf_train = np.delete(X_csf_train,0, 1 )\n",
    "X_csf_train = X_csf_train.astype(float)\n",
    "X_csf_test_ID = X_csf_test[:,0]\n",
    "X_csf_test = np.delete(X_csf_test,0, 1 )\n",
    "X_csf_test = X_csf_test.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b83ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also need training data with the ID in so that we can merge it later on \n",
    "y_csf_ID = np.column_stack((X_csf[:,0], y_csf))\n",
    "y_csf_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd64048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_csf = Sequential()\n",
    "model_csf.add(Dense(500, activation='tanh', input_dim=X_csf_train.shape[1]))\n",
    "model_csf.add(Dense(100, activation='tanh'))\n",
    "model_csf.add(Dense(50, activation='tanh'))\n",
    "model_csf.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_csf.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a162e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_csf.fit(X_csf_train, y_csf_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ac4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_csf= model_csf.predict(X_csf_train)\n",
    "scores_csf = model_csf.evaluate(X_csf_train, y_csf_train, verbose=0)\n",
    "print('csf Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_csf[1], 1 - scores_csf[1]))   \n",
    " \n",
    "pred_test_csf= model_csf.predict(X_csf_test)\n",
    "scores2_csf = model_csf.evaluate(X_csf_test, y_csf_test, verbose=0)\n",
    "print('csf Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_csf[1], 1 - scores2_csf[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model_uds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "csf_all_output = model_csf.predict(np.delete(X_csf,0, 1 ).astype(float))\n",
    "csf_all_output_ID = np.column_stack((X_csf[:,0], csf_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08286cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_csf_ID.shape) ; print(csf_all_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0075a6",
   "metadata": {},
   "source": [
    "## Merging the outputs for 2 stage of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to find a way to inner join the ID values and the softmax values from UDS model and MRI model / other tuples \n",
    "#do a concat with NACCID and the output and then do inner join?? \n",
    "#make data frames so can do pandas inner join then will change back to array \n",
    "pred_uds_IDs_df = pd.DataFrame(uds_all_output_ID, columns = [\"NACCID\", \"UDS_C1\", \"UDS_C2\", \"UDS_C3\"])\n",
    "pred_mri_IDs_df = pd.DataFrame(mri_all_output_ID, columns = [\"NACCID\", \"MRI_C1\", \"MRI_C2\", \"MRI_C3\"])\n",
    "pred_csf_IDs_df = pd.DataFrame(csf_all_output_ID, columns = [\"NACCID\", \"CSF_C1\", \"CSF_C2\", \"CSF_C3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frames with the softmax classifiers from the first stage of models \n",
    "X_uds_mri_ID = pd.merge(pred_uds_IDs_df, pred_mri_IDs_df, on=\"NACCID\", how=\"inner\")\n",
    "X_uds_csf_ID = pd.merge(pred_uds_IDs_df, pred_csf_IDs_df, on=\"NACCID\", how=\"inner\")\n",
    "X_mri_csf_ID = pd.merge(pred_mri_IDs_df, pred_csf_IDs_df, on=\"NACCID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to find a way to inner join the ID values and the softmax values from UDS model and MRI model / other tuples \n",
    "#do a concat with NACCID and the output and then do inner join?? \n",
    "#make data frames so can do pandas inner join then will change back to array \n",
    "y_uds_IDs_df = pd.DataFrame(y_uds_ID, columns = [\"NACCID\", \"ind_1\", \"ind_2\", \"ind_3\"])\n",
    "y_mri_IDs_df = pd.DataFrame(y_mri_ID, columns = [\"NACCID\", \"ind_1\", \"ind_2\", \"ind_3\"])\n",
    "y_csf_IDs_df = pd.DataFrame(y_csf_ID, columns = [\"NACCID\", \"ind_1\", \"ind_2\", \"ind_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frames with the softmax classifiers from the first stage of models \n",
    "y_uds_mri_ID = pd.merge(y_uds_IDs_df, y_mri_IDs_df[['NACCID']], on=\"NACCID\", how=\"inner\")\n",
    "y_uds_csf_ID = pd.merge(y_uds_IDs_df, y_csf_IDs_df[['NACCID']], on=\"NACCID\", how=\"inner\")\n",
    "y_mri_csf_ID = pd.merge(y_mri_IDs_df, y_csf_IDs_df[['NACCID']], on=\"NACCID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef584c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47307568",
   "metadata": {},
   "source": [
    "## Stage 2 \n",
    "\n",
    "### UDS / MRI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train for this model\n",
    "X_uds_mri_train, X_uds_mri_test, y_uds_mri_train, y_uds_mri_test = train_test_split(X_uds_mri_ID,\n",
    "                    y_uds_mri_ID, test_size=0.05,  random_state=20)\n",
    "print(X_uds_mri_train.shape); print(y_uds_mri_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert inputs to float arrays \n",
    "X_uds_mri_train = pd.DataFrame.to_numpy(X_uds_mri_train.iloc[: , 1:]) \n",
    "X_uds_mri_train = X_uds_mri_train.astype(float)\n",
    "y_uds_mri_train = pd.DataFrame.to_numpy(y_uds_mri_train.iloc[: , 1:]) \n",
    "y_uds_mri_train = y_uds_mri_train.astype(float)\n",
    "\n",
    "#convert inputs to float arrays \n",
    "X_uds_mri_test = pd.DataFrame.to_numpy(X_uds_mri_test.iloc[: , 1:]) \n",
    "X_uds_mri_test = X_uds_mri_test.astype(float)\n",
    "y_uds_mri_test = pd.DataFrame.to_numpy(y_uds_mri_test.iloc[: , 1:]) \n",
    "y_uds_mri_test = y_uds_mri_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de919f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uds_mri = Sequential()\n",
    "model_uds_mri.add(Dense(10, activation='tanh', input_dim=X_uds_mri_train.shape[1]))\n",
    "model_uds_mri.add(Dense(5, activation='tanh'))\n",
    "model_uds_mri.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_uds_mri.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755625ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uds_mri.fit(X_uds_mri_train, y_uds_mri_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_uds_mri= model_uds_mri.predict(X_uds_mri_train)\n",
    "scores_uds_mri = model_uds_mri.evaluate(X_uds_mri_train, y_uds_mri_train, verbose=0)\n",
    "print('uds mri Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_uds_mri[1], 1 - scores_uds_mri[1]))   \n",
    " \n",
    "pred_test_uds_mri= model_uds_mri.predict(X_uds_mri_test)\n",
    "scores2_uds_mri = model_uds_mri.evaluate(X_uds_mri_test, y_uds_mri_test, verbose=0)\n",
    "print('uds mri Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_uds_mri[1], 1 - scores2_uds_mri[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all data to numpy arrays \n",
    "X_uds_mri = pd.DataFrame.to_numpy(X_uds_mri_ID.iloc[: , 1:]) \n",
    "X_uds_mri = X_uds_mri.astype(float)\n",
    "                    \n",
    "               \n",
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "uds_mri_all_output = model_uds_mri.predict(X_uds_mri)\n",
    "uds_mri_all_output_ID = np.column_stack((X_uds_mri_ID.iloc[: , 0], uds_mri_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uds_mri_all_output_ID.shape) ; print(y_uds_mri_ID.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb484790",
   "metadata": {},
   "source": [
    "### UDS / CSF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20263369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train for this model\n",
    "X_uds_csf_train, X_uds_csf_test, y_uds_csf_train, y_uds_csf_test = train_test_split(X_uds_csf_ID,\n",
    "                    y_uds_csf_ID, test_size=0.05,  random_state=20)\n",
    "print(X_uds_csf_train.shape); print(y_uds_csf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert inputs to float arrays \n",
    "X_uds_csf_train = pd.DataFrame.to_numpy(X_uds_csf_train.iloc[: , 1:]) \n",
    "X_uds_csf_train = X_uds_csf_train.astype(float)\n",
    "y_uds_csf_train = pd.DataFrame.to_numpy(y_uds_csf_train.iloc[: , 1:]) \n",
    "y_uds_csf_train = y_uds_csf_train.astype(float)\n",
    "\n",
    "#convert inputs to float arrays \n",
    "X_uds_csf_test = pd.DataFrame.to_numpy(X_uds_csf_test.iloc[: , 1:]) \n",
    "X_uds_csf_test = X_uds_csf_test.astype(float)\n",
    "y_uds_csf_test = pd.DataFrame.to_numpy(y_uds_csf_test.iloc[: , 1:]) \n",
    "y_uds_csf_test = y_uds_csf_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uds_csf = Sequential()\n",
    "model_uds_csf.add(Dense(10, activation='tanh', input_dim=X_uds_csf_train.shape[1]))\n",
    "model_uds_csf.add(Dense(5, activation='tanh'))\n",
    "model_uds_csf.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_uds_csf.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45842939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uds_csf.fit(X_uds_csf_train, y_uds_csf_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_uds_csf= model_uds_csf.predict(X_uds_csf_train)\n",
    "scores_uds_csf = model_uds_csf.evaluate(X_uds_csf_train, y_uds_csf_train, verbose=0)\n",
    "print('uds csf Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_uds_csf[1], 1 - scores_uds_csf[1]))   \n",
    " \n",
    "pred_test_uds_csf= model_uds_csf.predict(X_uds_csf_test)\n",
    "scores2_uds_csf = model_uds_csf.evaluate(X_uds_csf_test, y_uds_csf_test, verbose=0)\n",
    "print('uds csf Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_uds_csf[1], 1 - scores2_uds_csf[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all data to numpy arrays \n",
    "X_uds_csf = pd.DataFrame.to_numpy(X_uds_csf_ID.iloc[: , 1:]) \n",
    "X_uds_csf = X_uds_csf.astype(float)\n",
    "                    \n",
    "               \n",
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "uds_csf_all_output = model_uds_csf.predict(X_uds_csf)\n",
    "uds_csf_all_output_ID = np.column_stack((X_uds_csf_ID.iloc[: , 0], uds_csf_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e260cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uds_csf_all_output_ID.shape) ; print(y_uds_csf_ID.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276bff6",
   "metadata": {},
   "source": [
    "### MRI / CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train for this model\n",
    "X_mri_csf_train, X_mri_csf_test, y_mri_csf_train, y_mri_csf_test = train_test_split(X_mri_csf_ID,\n",
    "                    y_mri_csf_ID, test_size=0.05,  random_state=20)\n",
    "print(X_mri_csf_train.shape); print(y_mri_csf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeaeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert inputs to float arrays \n",
    "X_mri_csf_train = pd.DataFrame.to_numpy(X_mri_csf_train.iloc[: , 1:]) \n",
    "X_mri_csf_train = X_mri_csf_train.astype(float)\n",
    "y_mri_csf_train = pd.DataFrame.to_numpy(y_mri_csf_train.iloc[: , 1:]) \n",
    "y_mri_csf_train = y_mri_csf_train.astype(float)\n",
    "\n",
    "#convert inputs to float arrays \n",
    "X_mri_csf_test = pd.DataFrame.to_numpy(X_mri_csf_test.iloc[: , 1:]) \n",
    "X_mri_csf_test = X_mri_csf_test.astype(float)\n",
    "y_mri_csf_test = pd.DataFrame.to_numpy(y_mri_csf_test.iloc[: , 1:]) \n",
    "y_mri_csf_test = y_mri_csf_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mri_csf = Sequential()\n",
    "model_mri_csf.add(Dense(10, activation='tanh', input_dim=X_mri_csf_train.shape[1]))\n",
    "model_mri_csf.add(Dense(5, activation='tanh'))\n",
    "model_mri_csf.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_mri_csf.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2dd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mri_csf.fit(X_mri_csf_train, y_mri_csf_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_mri_csf= model_mri_csf.predict(X_mri_csf_train)\n",
    "scores_mri_csf = model_mri_csf.evaluate(X_mri_csf_train, y_mri_csf_train, verbose=0)\n",
    "print('mri csf Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_mri_csf[1], 1 - scores_mri_csf[1]))   \n",
    " \n",
    "pred_test_mri_csf= model_mri_csf.predict(X_mri_csf_test)\n",
    "scores2_mri_csf = model_mri_csf.evaluate(X_mri_csf_test, y_mri_csf_test, verbose=0)\n",
    "print('mri csf Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_mri_csf[1], 1 - scores2_mri_csf[1]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all data to numpy arrays \n",
    "X_mri_csf = pd.DataFrame.to_numpy(X_mri_csf_ID.iloc[: , 1:]) \n",
    "X_mri_csf = X_mri_csf.astype(float)\n",
    "                    \n",
    "               \n",
    "#after training the network with our training test and testing set will put the whole data set into the model to give us \n",
    "#our softmax output for the next model \n",
    "mri_csf_all_output = model_mri_csf.predict(X_mri_csf)\n",
    "mri_csf_all_output_ID = np.column_stack((X_mri_csf_ID.iloc[: , 0], mri_csf_all_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0538411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mri_csf_all_output_ID.shape) ; print(y_mri_csf_ID.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a5841",
   "metadata": {},
   "source": [
    "## Stage 3 - UDS/ MRI/ CSF \n",
    "\n",
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to inner join all the output vectors based on the IDs \n",
    "#note that the output files are arrays and the y data is dataw frame\n",
    "#we want the input to the model to be an array (I think) so need to merge the inputs and outputs \n",
    "#then change them all to arrays \n",
    "pred_uds_mri_IDs_df = pd.DataFrame(uds_mri_all_output_ID, columns = [\"NACCID\", \"UM_1\", \"UM_C2\", \"UM_C3\"])\n",
    "pred_mri_csf_IDs_df = pd.DataFrame(mri_csf_all_output_ID, columns = [\"NACCID\", \"MC_C1\", \"MC_C2\", \"MC_C3\"])\n",
    "pred_uds_csf_IDs_df = pd.DataFrame(uds_csf_all_output_ID, columns = [\"NACCID\", \"UC_C1\", \"UC_C2\", \"UC_C3\"])\n",
    "\n",
    "#data frames with the softmax classifiers from the first stage of models \n",
    "X_UMC_ID = pd.merge(pd.merge(pred_mri_csf_IDs_df,pred_uds_csf_IDs_df,on=\"NACCID\", how=\"inner\"),pred_uds_mri_IDs_df, on=\"NACCID\", how=\"inner\")\n",
    "#X_UMC_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_UMC_ID = pd.merge(pd.merge(y_mri_csf_ID,y_uds_csf_ID,on=\"NACCID\", how=\"inner\"),y_uds_mri_ID, on=\"NACCID\", how=\"inner\")\n",
    "y_UMC_ID = y_UMC_ID.iloc[: , :4]\n",
    "#y_UMC_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00ac62",
   "metadata": {},
   "source": [
    "### UDS/ MRI/ CSF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train for this model\n",
    "X_UMC_train, X_UMC_test, y_UMC_train, y_UMC_test = train_test_split(X_UMC_ID,\n",
    "                    y_UMC_ID, test_size=0.05,  random_state=20)\n",
    "print(X_mri_csf_train.shape); print(y_mri_csf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert inputs to float arrays \n",
    "X_UMC_train = pd.DataFrame.to_numpy(X_UMC_train.iloc[: , 1:]) \n",
    "X_UMC_train = X_UMC_train.astype(float)\n",
    "y_UMC_train = pd.DataFrame.to_numpy(y_UMC_train.iloc[: , 1:]) \n",
    "y_UMC_train = y_UMC_train.astype(float)\n",
    "\n",
    "#convert inputs to float arrays \n",
    "X_UMC_test = pd.DataFrame.to_numpy(X_UMC_test.iloc[: , 1:]) \n",
    "X_UMC_test = X_UMC_test.astype(float)\n",
    "y_UMC_test = pd.DataFrame.to_numpy(y_UMC_test.iloc[: , 1:]) \n",
    "y_UMC_test = y_UMC_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_UMC = Sequential()\n",
    "model_UMC.add(Dense(12, activation='tanh', input_dim=X_UMC_train.shape[1]))\n",
    "model_UMC.add(Dense(6, activation='tanh'))\n",
    "model_UMC.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_UMC.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debe3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_UMC.fit(X_UMC_train, y_UMC_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_UMC= model_UMC.predict(X_UMC_train)\n",
    "scores_UMC = model_UMC.evaluate(X_UMC_train, y_UMC_train, verbose=0)\n",
    "print('mri csf Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_UMC[1], 1 - scores_UMC[1]))   \n",
    " \n",
    "pred_test_UMC= model_UMC.predict(X_UMC_test)\n",
    "scores2_UMC = model_UMC.evaluate(X_UMC_test, y_UMC_test, verbose=0)\n",
    "print('mri csf Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_UMC[1], 1 - scores2_UMC[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffb009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
